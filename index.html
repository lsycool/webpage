<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Shiyong Liu</title>

    <meta name="author" content="Shiyong Liu">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <style>
      /* 通用样式 */
      .media-container {
        display: flex;
        justify-content: center;
        align-items: center;
        position: relative;
        width: 100%;
        max-width: 300px;
        margin: 0 auto;
      }
      
      .media-container img,
      .media-container video {
        width: 100%;
        height: auto;
        display: block;
      }
      
      .overlay-media {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
      }

      /* 论文内容行容器：统一子元素顶端对齐 */
      .paper-content {
        display: flex;
        flex-direction: column;
      }
      
      /* 媒体容器外层：取消默认padding，与标题顶端对齐 */
      .media-wrapper {
        padding: 10px !important; /* 清除默认垂直padding，避免顶部偏移 */
      }
      
      /* 论文标题外层：取消默认padding，与媒体容器顶端对齐 */
      .title-wrapper {
        padding: 10px 15px !important; /* 仅保留水平padding，清除垂直padding */
      }

      /* 移动端响应式样式 */
      @media (max-width: 768px) {
        /* 基础样式调整 */
        body {
          padding: 0 5%;
        }
        
        /* 个人信息区域 */
        .name {
          font-size: 26px;
          padding-top: 15px;
        }
        
        /* 表格布局调整为垂直排列 */
        table, tbody, tr, td {
          display: block;
          width: 100% !important;
        }
        
        /* 头像区域优化 - 水平居中 */
        .avatar-container {
          display: flex;
          justify-content: center;
          margin: 0 auto 20px;
          width: 100% !important;
          padding: 2.5% 0 !important;
        }
        
        #avatar_lsy {
          margin: 0 auto;
          width: 200px !important;
          height: 200px !important;
        }
        
        #liushiyong_video, #liushiyong_image {
          width: 200px !important;
          height: 200px !important;
        }
        
        /* 研究论文区域：确保媒体容器与标题顶端对齐 */
        .one {
          width: 100% !important;
          height: auto !important;
          margin: 0 auto !important; /* 清除顶部margin，避免偏移 */
          padding: 0 !important; /* 清除内边距，与标题顶端对齐 */
        }
        
        /* 调整间距和字体 */
        td {
          padding: 10px 0 !important;
        }
        
        h2 {
          font-size: 20px;
          margin: 15px 0 10px;
        }
        
        p, a, strong {
          font-size: 13px;
          line-height: 1.6;
        }
        
        .papertitle {
          font-size: 15px;
          margin-top: 0 !important; /* 清除标题默认顶部margin */
        }
        
        /* 导航链接 */
        p:nth-of-type(3) {
          margin: 15px 0;
          line-height: 2;
        }

        /* 个人信息文本区域 */
        .personal-info {
          text-align: center;
          padding: 2.5% 0 !important;
        }

        /* 论文行容器：垂直排列且子元素顶端对齐 */
        .paper-row {
          display: flex;
          flex-direction: column-reverse;
          box-sizing: border-box;
          margin-bottom: 20px; /* 论文间增加间距，避免重叠 */
          padding-left: 10px;
          padding-right: 10px;
        }

        /* 限制视频容器最大高度并自动调整宽度保持比例 */
        .media-container {
          max-height: 200px; /* 根据需要调整合适高度 */
        }
        
        .media-container video,
        .media-container img {
          max-height: 100px;
          object-fit: contain; /* 保持视频/图片比例，避免拉伸 */
        }
        
      }
      /* 桌面端保持原有布局 + 顶端对齐 */
      @media (min-width: 769px) {
        .paper-row {
          display: table-row;
        }
        
        /* 桌面端：论文内容行横向排列，子元素顶端对齐 */
        .paper-content {
          display: table-row;
        }
        
        .media-wrapper, .title-wrapper {
          vertical-align: top !important; /* 强制子元素顶端对齐 */
        }
        
        .one {
          margin-top: 0 !important;
          padding-top: 0 !important;
        }
        
        .papertitle {
          margin-top: 0 !important;
        }
      }
    </style>
  </head>
  <body>
    <!-- 页面内容保持不变 -->
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <!-- 个人信息和头像区域 -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr style="padding:0px">
                <!-- 个人信息区域 - 添加类名用于样式控制 -->
                <td style="padding:2.5%;width:63%;vertical-align:middle" class="personal-info">
                  <p class="name" style="text-align: center;">
                    Shiyong Liu
                  </p>
                  <p>I'm a research scientist at <a href="https://www.dji.com/">DJI</a> in ShenZhen, China, where I work on a small team that mostly works on 4D World Model.
                  </p>
                  <p>
                    I have experience in Large Scene Reconstruction (combining UAV and ground-based capture), Camera Pose Estimation, Motion Capture, VR, Multi-modal Recommendation & Search, Defect Detection, and Machine Vision. I earned my Master's degree from <a href="https://www.nudt.edu.cn">National University of Defense Technology (NUDT)</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:lsy97_cug@163.com">Email</a> &nbsp;/&nbsp;
                    <a href="http://liushiyong.cn/"><b>EN</b><span style="font-size: 10px;">/</span>CN</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?pli=1&authuser=1&user=3mIbHHkAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/lsycool/">Github</a>
                  </p>
                </td>
                
                <!-- 头像区域 - 添加类名用于居中控制 -->
                <td style="padding:2.5%;width:37%" class="avatar-container">
                  <div id='avatar_lsy' style="display: flex; align-items: center; justify-content: center; width: 251px; height: 251px; position: relative; cursor: pointer;" onmouseout="liushiyong_stop()" onmouseover="liushiyong_start()">
                    <div id='liushiyong_video' style="width: 251px; height: 251px; position: absolute;" class="overlay-media">
                      <video  id="digital_lsy" style="border-radius: 50%;" width=100% height=100% playsinline autoplay>
                        <source src="images/liushiyong.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                      </video>
                    </div>
                    <img style="width:100%; max-width:100%; object-fit: cover; border-radius: 50%;" src='images/liushiyong.jpg' id='liushiyong_image' width=100% class="hoverZoomLink">
                  </div>
                  <script type="text/javascript">
                    function liushiyong_start() {
                      document.getElementById('liushiyong_video').style.opacity = "1";
                      document.getElementById('liushiyong_image').style.opacity = "0";
                    }

                    function liushiyong_stop() {
                      document.getElementById('liushiyong_video').style.opacity = "0";
                      document.getElementById('liushiyong_image').style.opacity = "1";
                    }
                    liushiyong_stop()

                    const playButton = document.getElementById('avatar_lsy');
                    const video = document.getElementById('digital_lsy');
                    playButton.addEventListener('click', function () {
                      if (video.paused) {
                        video.play();
                      } else {
                        video.pause();
                      }
                    });
                  </script>
                </td>
              </tr>
            </tbody></table>
            
            <!-- 研究内容区域 -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm interested in 3D computer vision, including camera pose estimation, motion capture and 3D reconstruction. Some papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody></table>
            
            <!-- 论文列表（保持不变） -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- 论文1: OccluGaussian -->
              <tr onmouseout="ocllugs_stop()" onmouseover="ocllugs_start()" bgcolor="#ffffd0" class="paper-row">
                <!-- 媒体容器外层：添加.media-wrapper确保顶端对齐 -->
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='occlugaussian_video'>
                        <video muted autoplay loop>
                          <source src="images/occlugaussian.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src='images/occlugaussian.jpg' id='occlugaussian_image' width=100%>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function ocllugs_start() {
                      document.getElementById('occlugaussian_video').style.opacity = "1";
                      document.getElementById('occlugaussian_image').style.opacity = "0";
                    }

                    function ocllugs_stop() {
                      document.getElementById('occlugaussian_video').style.opacity = "0";
                      document.getElementById('occlugaussian_image').style.opacity = "1";
                    }
                    ocllugs_stop()
                  </script>
                </td>
                <!-- 论文标题外层：添加.title-wrapper确保顶端对齐 -->
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2503.16177">
                    <span class="papertitle">OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering</span>
                  </a>
                  <br>
                  <strong>Shiyong Liu</strong>,
                  <a>Xiao Tang</a>,
                  <a>Zhihao Li</a>,
                  <a>Yingfan He</a>
                  <a>ChongJie Ye</a>,
                  <a>Jianzhuang Liu</a>,
                  <a>Binxiao Huang</a>
                  <a>Shunbo Zhou</a>
                  <a>Xiaofei Wu</a>
                  <br>
                  <em>ICCV</em>, 2025
                  <br>
                  <a href="https://occlugaussian.github.io">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2503.16177">arXiv</a>
                  <p></p>
                  <p>
                    A method for accelerating and enhancing large-scale 3D scene reconstruction via occlusion-aware camera clustering and region-based Gaussian culling.
                  </p>
                </td>
              </tr>

              <!-- 论文2: Direction-Aware Hybrid Representation Learning -->
              <tr onmouseout="dahyf_stop()" onmouseover="dahyf_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='dahyf_video'>
                        <video muted autoplay loop>
                          <source src="images/dahyf.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src='images/dahyf.png' id='dahyf_image' width=100%>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function dahyf_start() {
                      document.getElementById('dahyf_video').style.opacity = "1";
                      document.getElementById('dahyf_image').style.opacity = "0";
                    }

                    function dahyf_stop() {
                      document.getElementById('dahyf_video').style.opacity = "0";
                      document.getElementById('dahyf_image').style.opacity = "1";
                    }
                    dahyf_stop()
                  </script>
                </td>
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2504.01298">
                    <span class="papertitle">Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation</span>
                  </a>
                  <br>
                  <strong>Shiyong Liu</strong>,
                  <a>Zhihao Li</a>,
                  <a>Xiao Tang</a>,
                  <a>Jianzhuang Liu</a>,
                  <br>
                  <em>CVPRW</em>, 2025, 
                  <span class="glowing-text">
                    <b>First Place in HO3Dv2/v3 leaderboards for the metric of PA-MPJPE</b>
                  </span>
                  <br>
                  <a href="">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2504.01298">arXiv</a>
                  <p></p>
                  <p>
                    A method for 3D hand pose/shape estimation, fusing implicit image/explicit 2D joint features with pixel direction info, reducing motion jitter via confidence-based prediction.
                  </p>
                </td>
              </tr>

              <!-- 论文3: SpecTRe -->
              <tr onmouseout="SpecTRe_stop()" onmouseover="SpecTRe_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='SpecTRe_video'>
                        <video muted autoplay loop>
                          <source src="images/SpecTRe.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/SpecTRe.svg' id='SpecTRe_image' width=100%>
					</div>
                  </div>
                  <script type="text/javascript">
                    function SpecTRe_start() {
                      document.getElementById('SpecTRe_video').style.opacity = "1";
                      document.getElementById('SpecTRe_image').style.opacity = "0";
                    }

                    function SpecTRe_stop() {
                      document.getElementById('SpecTRe_video').style.opacity = "0";
                      document.getElementById('SpecTRe_image').style.opacity = "1";
                    }
                    SpecTRe_stop()
                  </script>
                </td>
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">SpecTRe-GS: Modeling Highly Specular Surfaces with Reflected Nearby Objects by Tracing Rays in 3D Gaussian Splatting</span>
                  </a>
                  <br>
                  <a>Jiajun Tang</a>,
                  <a>Fan Fei</a>,
                  <a>Zhihao Li</a>,
                  <a>Xiao Tang</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Youyu Chen</a>,
                  <a>Binxiao Huang</a>,
                  <a>Dave Zhenyu Chen</a>,
                  <a>Xiaofei Wu</a>,
                  <a>Boxin Shi</a>
                  <br>
                  <em>CVPR</em>, 2025,
                  <span class="glowing-text">
                    <b>Highlight</b>
                  </span>
                  <br>
                  <a href="https://spectre-gs.github.io/">project page</a>
                  /
                  <a href="">arXiv</a>
                  <p></p>
                  <p>
                    A method for enhancing 3DGS with ray tracing for specular inter-reflections, separating specular/rough surface types and optimizing geometry to improve rendering accuracy and enable scene editing.</p>
                </td>
              </tr>

              <!-- 论文4: Hybrid -->
              <tr onmouseout="Hybrid_stop()" onmouseover="Hybrid_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='Hybrid_video'>
                        <video muted autoplay loop>
                          <source src="images/Hybrid.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                      	</video>
                      </div>
                      <img src='images/Hybrid.png' id='Hybrid_image' width=100%>
					</div>
                  </div>
                  <script type="text/javascript">
                    function Hybrid_start() {
                      document.getElementById('Hybrid_video').style.opacity = "1";
                      document.getElementById('Hybrid_image').style.opacity = "0";
                    }

                    function Hybrid_stop() {
                      document.getElementById('Hybrid_video').style.opacity = "0";
                      document.getElementById('Hybrid_image').style.opacity = "1";
                    }
                    Hybrid_stop()
                  </script>
                </td>
                
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction</span>
                  </a>
                  <br>
                  <a>Binxiao Huang</a>,
                  <a>Zhihao Li</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Xiao Tang</a>,
                  <a>Jiajun Tang</a>,
                  <a>Jiaqi Lin</a>,
                  <a>Yuxin Cheng</a>,
                  <a>Zhenyu Chen</a>,
                  <a>Xiaofei Wu</a>,
                  <a>Ngai Wong</a>
                  <br>
                  <em>IJCAI</em>, 2025
                  <br>
                  <a href="">project page</a>
                  /
                  <a href="https://www.arxiv.org/pdf/2506.06988">arXiv</a>
                  <p></p>
                  <p>
                    A hybrid 3DGS and textured mesh representation, where meshes handle texture-rich flat regions and Gaussians model complex geometries, optimized via joint training with a warm-up strategy and transmittance-aware supervision, achieving comparable rendering quality at higher FPS with fewer Gaussians.</p>
                </td>
              </tr>


              <tr onmouseout="zeropto3_stop()" onmouseover="zeropto3_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='zeropto3_video'>
                        <video muted autoplay loop>
                          <source src="images/zeropto3.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                      	</video>
                      </div>
                      <img src='images/zeropto3.png' id='zeropto3_image' width=100%>
					</div>
                  </div>
                  <script type="text/javascript">
                    function zeropto3_start() {
                      document.getElementById('zeropto3_video').style.opacity = "1";
                      document.getElementById('zeropto3_image').style.opacity = "0";
                    }

                    function zeropto3_stop() {
                      document.getElementById('zeropto3_video').style.opacity = "0";
                      document.getElementById('zeropto3_image').style.opacity = "1";
                    }
                    zeropto3_stop()
                  </script>
                </td>
                
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object</span>
                  </a>
                  <br>
                  <a>Yuxuan Lin</a>,
                  <a>Zhihao Li</a>,
                  <a>Ruihang Chu</a>,
                  <a>Zhenyu Chen</a>,
                  <a>Xiao Tang</a>,
                  <a>Lei Ke</a>,
                  <a>Haoling Li</a>,
                  <a>Yingji Zhong</a>,
                  <a>Zhihao Li</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Xiaofei Wu</a>,
                  <a>Jianzhuang Liu</a>,
                  <a>Yujiu Yang</a>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2505.23054">arXiv</a>
                  <p></p>
                  <p>
                    Generative 3D reconstruction in partial observation, hindered by limited view range and inconsistent generation, is advanced by the training-free Zero-P-to-3, which integrates local dense observations and multi-source priors via fusion-based DDIM sampling and iterative refinement, outperforming SOTAs especially in invisible regions.                
                  </p>
                  </td>
              </tr>

              <!-- 论文6: davigs -->
              <tr onmouseout="davigs_stop()" onmouseover="davigs_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='davigs_video'>
                        <video muted autoplay loop>
                          <source src="images/DAVIGS.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src='images/DAVIGS.png' id='davigs_image' width=100%>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function davigs_start() {
                      document.getElementById('davigs_video').style.opacity = "1";
                      document.getElementById('davigs_image').style.opacity = "0";
                    }

                    function davigs_stop() {
                      document.getElementById('davigs_video').style.opacity = "0";
                      document.getElementById('davigs_image').style.opacity = "1";
                    }
                    davigs_stop()
                  </script>
                </td>
                
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2501.10788">
                    <span class="papertitle">Decoupling Appearance Variations with 3D Consistent Features in Gaussian Splatting</span>
                  </a>
                  <br>
                  <a>Jiaqi Ling</a>,
                  <a>Zhihao Li</a>,
                  <a>Bingxiao Huang</a>,
                  <a>Xiao Tang</a>
                  <a>Jianzhuang Liu</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Xiaofei Wu</a>
                  <a>Fenglong Song</a>
                  <a>Wenming Yang</a>
                  <br>
                  <em>AAAI</em>, 2025
                  <br>
                  <a href="https://davi-gaussian.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2501.10788">arXiv</a>
                  <p></p>
                  <p>
                    A method for decoupling appearance variations in a plug-and-play and efficient manner.
                  </p>
                </td>
              </tr>

              <tr onmouseout="vase_stop()" onmouseover="vase_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='vastgaussian_video'>
                        <video muted autoplay loop>
                          <source src="images/vastgaussian.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src='images/vastgaussian.png' id='vastgaussian_image' width=100%>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function vase_start() {
                      document.getElementById('vastgaussian_video').style.opacity = "1";
                      document.getElementById('vastgaussian_image').style.opacity = "0";
                    }

                    function vase_stop() {
                      document.getElementById('vastgaussian_video').style.opacity = "0";
                      document.getElementById('vastgaussian_image').style.opacity = "1";
                    }
                    vase_stop()
                  </script>
                </td>
                
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2402.17427">
                <span class="papertitle">Vastgaussian: Vast 3d gaussians for large scene reconstruction</span>
                  </a>
                  <br>
                  <a>Jiaqi Ling</a>,
                  <a>Zhihao Li</a>,
                  <a>Jianzhuang Liu</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Yangdi Lu</a>,
                  <a>Xiaofei Wu</a>
                  <a>Songcen Xu</a>
                  <a>Youliang Yan</a>
                  <a>Wenming Yang</a>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://vastgaussian.github.io">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2402.17427">arXiv</a>
                  <p></p>
                  <p>
                  An approach for large scene reconstruction that enabling fast optimization and high-fidelity real-time rendering.
                  </p>
                </td>
              </tr>

              <tr onmouseout="ever_stop()" onmouseover="ever_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='mirror_video'>
                        <video muted autoplay loop>
                          <source src="images/mirrorgaussian.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src='images/mirrorgaussian.png' id='mirror_image' width=100%>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function ever_start() {
                      document.getElementById('mirror_video').style.opacity = "1";
                      document.getElementById('mirror_image').style.opacity = "0";
                    }

                    function ever_stop() {
                      document.getElementById('mirror_video').style.opacity = "0";
                      document.getElementById('mirror_image').style.opacity = "1";
                    }
                    ever_stop()
                  </script>
                </td>
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2405.11921">
                <span class="papertitle">Mirrorgaussian: Reflecting 3d gaussians for reconstructing mirror reflections</span>
                  </a>
                  <br>
                  <a>Jiayue Liu</a>, 
                  <a>Xiao Tang</a>,
                  <a>Freeman Cheng</a>,
                  <a>Roy Yang</a>,
                  <a>Zhihao Li</a>,
                  <a>Jianzhuang Liu</a>,
                  <a>Yi Huang</a>,
                  <a>Jiaqi Lin</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Xiaofei Wu</a>
                  <a>Songcen Xu</a>
                  <a>Chun Yuan</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://mirror-gaussian.github.io">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2405.11921">arXiv</a>
                  <p></p>
                  <p>
                    MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects.
                  </p>
                </td>
              </tr>

              <tr onmouseout="unorthogonalized_stop()" onmouseover="unorthogonalized_start()" bgcolor="#ffffd0" class="paper-row">
                <td class="media-wrapper" style="width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="media-container">
                      <div class="overlay-media" id='unorthogonalized_video'>
                        <video muted autoplay loop>
                          <source src="images/unorthogonalized.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src='images/unorthogonalized.png' id='unorthogonalized_image' width=100%>
                    </div>
                  </div>
                  <script type="text/javascript">
                    function unorthogonalized_start() {
                      document.getElementById('unorthogonalized_video').style.opacity = "1";
                      document.getElementById('unorthogonalized_image').style.opacity = "0";
                    }

                    function unorthogonalized_stop() {
                      document.getElementById('unorthogonalized_video').style.opacity = "0";
                      document.getElementById('unorthogonalized_image').style.opacity = "1";
                    }
                    unorthogonalized_stop()
                  </script>
                </td>
                <td class="title-wrapper" style="width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2312.00462">
                <span class="papertitle">LEARNING UNORTHOGONALIZED MATRICES FOR ROTATION ESTIMATION</span>
                  </a>
                  <br>
                  <a>Kerui Gu</a>, 
                  <a>Zhihao Li</a>,
                  <strong>Shiyong Liu</strong>,
                  <a>Jianzhuang Liu</a>,
                  <a>Songcen Xu</a>,
                  <a>Youliang Yan</a>,
                  <a>Michael Bi Mi</a>,
                  <a>Kenji Kawaguchi</a>,
                  <a>Angela Yao</a>
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2312.00462">arXiv</a>
                  <p></p>
                  <p>
                    Estimating 3D rotations relies on rotation representations, and this work reveals that common orthogonalization procedures slow training efficiency, thus advocating learning unorthogonalized Pseudo Rotation Matrices (PRoM) which converge faster to better solutions and achieve state-of-the-art results in human pose estimation on large-scale benchmarks.
                  </p>
                </td>
              </tr>
            </tbody></table>
            
            <!-- 其他内容区域 -->
            <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody></table>
            
            <!-- 行业项目区域 -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                    <div class="colored-box" style="background-color: #edd892;">
                    <h2>Industry Projects</h2>
                    </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a>3DGS reconstruction on mobile phones</a>
                  <br>
                  <a>Large scene reconstruction and real-time rendering</a>
                  <br>
                  <a>Large scene camera pose estimation</a>
                  <br>
                  <a>3D motion capture for digital human</a>
                  <br>
                  <a>Multimodal video-based content search technology</a>
                  <br>
                  <a>Leading Huawei’s iVision platform and its operators</a>
                  <br>
                  <a>Responsible for industrial defect detection.</a>
                </td>
              </tr>
            </tbody></table>
            
            <!-- 页脚 -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Thanks for Jon Barron's website open source
                    <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. 
                  </p>
                </td>
              </tr>
            </tbody></table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
